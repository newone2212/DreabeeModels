{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the page containing the list of cities\n",
    "url = 'https://www.britannica.com/topic/list-of-cities-and-towns-in-India-2033033'\n",
    "\n",
    "# Send a request to the website\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all city names\n",
    "city_elements = soup.find_all('a', class_='md-crosslink')\n",
    "\n",
    "# Extract city names\n",
    "cities = [city.get_text() for city in city_elements]\n",
    "\n",
    "# Save the city names to a CSV file\n",
    "cities_df = pd.DataFrame(cities, columns=['City'])\n",
    "cities_df.to_csv('all_india_cities.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "genrating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Load country and city data from the uploaded CSV file\n",
    "file_path = 'country_data.csv'\n",
    "df_world = pd.read_csv(file_path)\n",
    "\n",
    "brands_df = pd.read_csv('brands_only (1).csv')\n",
    "brands = brands_df['Brand'].tolist()\n",
    "# Define brand categories\n",
    "brand_categories = [\n",
    "    'Film & Animation', 'Autos & Vehicles', 'Music', 'Pets & Animals', 'Sports', 'Short Movies', 'Travel & Events',\n",
    "    'Gaming', 'Videoblogging', 'People & Blogs', 'Comedy', 'Entertainment', 'News & Politics', 'Howto & Style',\n",
    "    'Education', 'Science & Technology', 'Nonprofits & Activism', 'Movies', 'Anime/Animation', 'Action/Adventure',\n",
    "    'Classics', 'Comedy', 'Documentary', 'Drama', 'Family', 'Foreign', 'Horror', 'Sci-Fi/Fantasy', 'Thriller', 'Shorts',\n",
    "    'Shows', 'Trailers'\n",
    "]\n",
    "\n",
    "# Function to generate random view percentages for countries\n",
    "def generate_country_view_percentages(num_countries):\n",
    "    percentages = [random.randint(10, 50) for _ in range(num_countries)]\n",
    "    total = sum(percentages)\n",
    "    normalized_percentages = [round(p / total * 100, 2) for p in percentages]\n",
    "    return normalized_percentages\n",
    "\n",
    "# # Sample brands (replace with actual brand names)\n",
    "# brands = ['Brand1', 'Brand2', 'Brand3']\n",
    "\n",
    "# Generate the dataset\n",
    "data = []\n",
    "max_rows_per_file = 100000  # Adjust this value based on your requirements\n",
    "\n",
    "file_counter = 1\n",
    "row_counter = 0\n",
    "\n",
    "for brand in brands:\n",
    "    for brand_category in brand_categories:\n",
    "        countries = df_world['country'].unique()\n",
    "        country_percentages = generate_country_view_percentages(len(countries))\n",
    "        \n",
    "        for country, total_percentage in zip(countries, country_percentages):\n",
    "            country_cities = df_world[df_world['country'] == country]['city'].tolist()\n",
    "            num_cities = len(country_cities)\n",
    "            if num_cities > 0:\n",
    "                # Generate random percentages for cities that sum up to the country's percentage\n",
    "                city_percentages = [random.uniform(0.5, 1.5) for _ in range(num_cities)]\n",
    "                city_percentages_total = sum(city_percentages)\n",
    "                normalized_city_percentages = [round((p / city_percentages_total) * total_percentage, 2) for p in city_percentages]\n",
    "\n",
    "                # Adjust the first city percentage to ensure the total percentage is correct\n",
    "                diff = total_percentage - sum(normalized_city_percentages)\n",
    "                normalized_city_percentages[0] += diff\n",
    "\n",
    "                for city, percentage in zip(country_cities, normalized_city_percentages):\n",
    "                    row = [brand, brand_category, country, city, percentage]\n",
    "                    data.append(row)\n",
    "                    row_counter += 1\n",
    "                    \n",
    "                    # Check if the number of rows exceeds the limit\n",
    "                    if row_counter >= max_rows_per_file:\n",
    "                        # Save the current dataset to a CSV file\n",
    "                        columns = ['Brand', 'Brand Category', 'Country', 'City', 'View Percentage']\n",
    "                        dummy_df = pd.DataFrame(data, columns=columns)\n",
    "                        dummy_df.to_csv(f'dummy_city_view_percentages_part_{file_counter}.csv', index=False)\n",
    "                        \n",
    "                        # Reset data and counters\n",
    "                        data = []\n",
    "                        row_counter = 0\n",
    "                        file_counter += 1\n",
    "\n",
    "# Save any remaining data to a CSV file\n",
    "if data:\n",
    "    columns = ['Brand', 'Brand Category', 'Country', 'City', 'View Percentage']\n",
    "    dummy_df = pd.DataFrame(data, columns=columns)\n",
    "    dummy_df.to_csv(f'dummy_city_view_percentages_part_{file_counter}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing-data-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Function to distribute a country's percentage among its cities\n",
    "def distribute_percentage_among_cities(total_percentage, num_cities):\n",
    "    if num_cities == 1:\n",
    "        return [total_percentage]\n",
    "    \n",
    "    # Create random values and normalize them to sum up to 1\n",
    "    random_values = [random.uniform(0.5, 1.5) for _ in range(num_cities)]\n",
    "    random_total = sum(random_values)\n",
    "    \n",
    "    # Assign city percentages based on random values, scaled to the total_percentage\n",
    "    city_percentages = [round((value / random_total) * total_percentage, 2) for value in random_values]\n",
    "    \n",
    "    # Adjust the last city's percentage to ensure the sum equals the total_percentage\n",
    "    difference = total_percentage - sum(city_percentages)\n",
    "    city_percentages[-1] += difference\n",
    "    \n",
    "    return city_percentages\n",
    "\n",
    "# Directory containing the generated CSV files\n",
    "directory = \"dataset_for_country_cities\"  # Replace with the actual path to your 290 files\n",
    "\n",
    "# Process each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        \n",
    "        # Load the data\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Iterate over each combination of Brand, Brand Category, and Country\n",
    "        updated_rows = []\n",
    "        for (brand, brand_category, country), group in df.groupby(['Brand', 'Brand Category', 'Country']):\n",
    "            # Assign a random percentage to the country for the specific brand and category\n",
    "            country_percentage = random.randint(10, 50)\n",
    "            num_cities = len(group)\n",
    "            \n",
    "            # Distribute the country percentage among the cities\n",
    "            city_percentages = distribute_percentage_among_cities(country_percentage, num_cities)\n",
    "            \n",
    "            for i, (index, row) in enumerate(group.iterrows()):\n",
    "                row['Country Percentage'] = country_percentage\n",
    "                row['City Percentage'] = city_percentages[i]\n",
    "                updated_rows.append(row)\n",
    "        \n",
    "        # Save the updated data back to the file\n",
    "        updated_df = pd.DataFrame(updated_rows)\n",
    "        updated_df.to_csv(file_path, index=False)\n",
    "\n",
    "print(\"City and country percentage columns added and saved for all files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encodeing Data (Prepreocessing -step-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import joblib\n",
    "\n",
    "# Directory containing the generated CSV files\n",
    "directory = \"dataset_for_country_cities\"  # Replace with the actual path to your 290 files\n",
    "\n",
    "# Initialize an empty list to store all the data for fitting the encoder\n",
    "all_data = []\n",
    "\n",
    "# Load all data to fit the encoder\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        print(f\"Loading data for encoder fitting: {file_path}\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        all_data.append(df[['Brand', 'Brand Category', 'Country', 'City', 'Country Percentage']])\n",
    "\n",
    "# Combine all the data into a single DataFrame\n",
    "df_all = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Fit the encoder on the entire dataset\n",
    "encoder.fit(df_all)\n",
    "\n",
    "# Save the encoder for later use\n",
    "encoder_path = 'ml_models/models/cities_model/encoder.pkl'\n",
    "joblib.dump(encoder, encoder_path)\n",
    "print(f\"Encoder saved to {encoder_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "features and target PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import joblib\n",
    "import scipy.sparse\n",
    "\n",
    "# Directory containing the generated CSV files\n",
    "directory = \"dataset_for_country_cities\"  # Replace with the actual path to your 290 files\n",
    "\n",
    "# Define the chunk size\n",
    "chunk_size = 50000  # Adjust this value based on your memory limitations\n",
    "\n",
    "# Initialize empty lists to store encoded chunks and target chunks\n",
    "encoded_chunks = []\n",
    "target_chunks = []\n",
    "\n",
    "# Process each file in the directory and encode using the fitted encoder\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        \n",
    "        # Load the data in chunks\n",
    "        for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "            X = chunk[['Brand', 'Brand Category', 'Country', 'City', 'Country Percentage']]\n",
    "            y = chunk['City Percentage']\n",
    "            \n",
    "            # Transform the features using the already fitted encoder\n",
    "            encoded_chunk = encoder.transform(X)\n",
    "            encoded_chunks.append(encoded_chunk)\n",
    "            target_chunks.append(y)\n",
    "\n",
    "# Save the encoded chunks and target chunks separately\n",
    "encoded_data_path = 'ml_models/models/cities_model/encoded_features.pkl'\n",
    "target_data_path = 'ml_models/models/cities_model/target.pkl'\n",
    "joblib.dump(encoded_chunks, encoded_data_path)\n",
    "joblib.dump(target_chunks, target_data_path)\n",
    "print(f\"Encoded data saved to {encoded_data_path}\")\n",
    "print(f\"Target data saved to {target_data_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import scipy.sparse\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Load the encoded features and targets\n",
    "encoded_data_path = 'ml_models/models/cities_model/encoded_features.pkl'\n",
    "target_data_path = 'ml_models/models/cities_model/target.pkl'\n",
    "\n",
    "encoded_chunks = joblib.load(encoded_data_path)\n",
    "target_chunks = joblib.load(target_data_path)\n",
    "\n",
    "# Combine the encoded chunks and target chunks\n",
    "X = scipy.sparse.vstack(encoded_chunks)  # Keep it as a sparse matrix\n",
    "y = pd.concat(target_chunks, ignore_index=True)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the SGDRegressor model\n",
    "model = SGDRegressor(\n",
    "    max_iter=100,       # Single pass through the data for each chunk\n",
    "    tol=None,         # Disable early stopping (for demonstration)\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# # Chunked training approach\n",
    "# chunk_size = 10000  # Adjust based on your memory capacity\n",
    "# n_chunks = X_train.shape[0] // chunk_size\n",
    "\n",
    "# for i in range(n_chunks):\n",
    "#     start = i * chunk_size\n",
    "#     end = start + chunk_size\n",
    "#     X_chunk = X_train[start:end]\n",
    "#     y_chunk = y_train[start:end]\n",
    "\n",
    "#     # Fit the model on the chunk using partial_fit\n",
    "#     model.partial_fit(X_chunk, y_chunk)\n",
    "\n",
    "# Train the model on the entire dataset\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "score = r2_score(y_test, y_pred)\n",
    "print(f\"Model R^2 score on test data: {score:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "model_path = 'ml_models/models/cities_model/city_percentage_model.pkl'\n",
    "joblib.dump(model, model_path)\n",
    "print(f\"Model saved to {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import scipy.sparse\n",
    "\n",
    "# Load the encoded features and targets\n",
    "encoded_data_path = 'ml_models/models/cities_model/encoded_features.pkl'\n",
    "target_data_path = 'ml_models/models/cities_model/target.pkl'\n",
    "\n",
    "encoded_chunks = joblib.load(encoded_data_path)\n",
    "target_chunks = joblib.load(target_data_path)\n",
    "\n",
    "# Combine the encoded chunks and target chunks\n",
    "X = scipy.sparse.vstack(encoded_chunks)  # Assuming the data is sparse\n",
    "y = pd.concat(target_chunks, ignore_index=True)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "score = r2_score(y_test, y_pred)\n",
    "print(f\"Model R^2 score on test data: {score:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "model_path = 'ml_models/models/cities_model/city_percentage_model.pkl'\n",
    "joblib.dump(model, model_path)\n",
    "print(f\"Model saved to {model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Load the list of cities\n",
    "cities_df = pd.read_csv('all_india_cities.csv')\n",
    "cities = cities_df['City'].tolist()\n",
    "\n",
    "# List of brand categories\n",
    "brand_categories = [\n",
    "    'Film & Animation', 'Autos & Vehicles', 'Music', 'Pets & Animals', 'Sports', 'Short Movies', 'Travel & Events',\n",
    "    'Gaming', 'Videoblogging', 'People & Blogs', 'Comedy', 'Entertainment', 'News & Politics', 'Howto & Style',\n",
    "    'Education', 'Science & Technology', 'Nonprofits & Activism', 'Movies', 'Anime/Animation', 'Action/Adventure',\n",
    "    'Classics', 'Comedy', 'Documentary', 'Drama', 'Family', 'Foreign', 'Horror', 'Sci-Fi/Fantasy', 'Thriller', 'Shorts',\n",
    "    'Shows', 'Trailers'\n",
    "]\n",
    "\n",
    "# Load the brands from the provided CSV file\n",
    "brands_df = pd.read_csv('brands_only (1).csv')\n",
    "brands = brands_df['Brand'].tolist()\n",
    "\n",
    "# Function to generate random view percentages\n",
    "def generate_view_percentages():\n",
    "    percentages = [random.randint(5, 30) for _ in range(len(cities))]\n",
    "    total = sum(percentages)\n",
    "    return [round(p / total * 100, 2) for p in percentages]\n",
    "\n",
    "# Generate the dummy dataset\n",
    "data = []\n",
    "\n",
    "for brand in brands:\n",
    "    brand_category = random.choice(brand_categories)\n",
    "    percentages = generate_view_percentages()\n",
    "    for city, percentage in zip(cities, percentages):\n",
    "        row = [brand, brand_category, city, percentage]\n",
    "        data.append(row)\n",
    "\n",
    "# Create a DataFrame\n",
    "columns = ['Brand', 'Brand Category', 'City', 'View Percentage']\n",
    "dummy_df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Save the dummy dataset to a CSV file\n",
    "dummy_df.to_csv('dummy_city_view_percentages_with_all_cities.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1/22...\n",
      "Processing chunk 2/22...\n",
      "Processing chunk 3/22...\n",
      "Processing chunk 4/22...\n",
      "Processing chunk 5/22...\n",
      "Processing chunk 6/22...\n",
      "Processing chunk 7/22...\n",
      "Processing chunk 8/22...\n",
      "Processing chunk 9/22...\n",
      "Processing chunk 10/22...\n",
      "Processing chunk 11/22...\n",
      "Processing chunk 12/22...\n",
      "Processing chunk 13/22...\n",
      "Processing chunk 14/22...\n",
      "Processing chunk 15/22...\n",
      "Processing chunk 16/22...\n",
      "Processing chunk 17/22...\n",
      "Processing chunk 18/22...\n",
      "Processing chunk 19/22...\n",
      "Processing chunk 20/22...\n",
      "Processing chunk 21/22...\n",
      "Processing chunk 22/22...\n",
      "Data encoding and saving complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import joblib\n",
    "\n",
    "# Load the generated dataset\n",
    "df = pd.read_csv('dummy_city_view_percentages_with_all_cities.csv')\n",
    "\n",
    "# Split the data into smaller chunks\n",
    "chunk_size = 50000  # Adjust the chunk size based on your memory limitations\n",
    "num_chunks = len(df) // chunk_size + 1\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "encoder.fit(df[['Brand', 'Brand Category', 'City']])\n",
    "\n",
    "# Save the encoder\n",
    "joblib.dump(encoder, 'ml_models/models/cities_model/encoder.pkl')\n",
    "\n",
    "# Process each chunk separately\n",
    "for i in range(num_chunks):\n",
    "    print(f\"Processing chunk {i+1}/{num_chunks}...\")\n",
    "    chunk = df[i*chunk_size:(i+1)*chunk_size]\n",
    "    \n",
    "    # Encode the categorical features\n",
    "    encoded_features = encoder.transform(chunk[['Brand', 'Brand Category', 'City']])\n",
    "    \n",
    "    # Save the encoded features and target\n",
    "    joblib.dump(encoded_features, f'ml_models/models/cities_model/encoded_features_chunk_{i+1}.pkl')\n",
    "    joblib.dump(chunk['View Percentage'], f'ml_models/models/cities_model/target_chunk_{i+1}.pkl')\n",
    "\n",
    "print(\"Data encoding and saving complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on chunk 1/22...\n",
      "Chunk 1 - Mean Squared Error: 0.006640443061600881\n",
      "Training on chunk 2/22...\n",
      "Chunk 2 - Mean Squared Error: 0.006484119164189637\n",
      "Training on chunk 3/22...\n",
      "Chunk 3 - Mean Squared Error: 0.006301960383437895\n",
      "Training on chunk 4/22...\n",
      "Chunk 4 - Mean Squared Error: 0.0062875247738416375\n",
      "Training on chunk 5/22...\n",
      "Chunk 5 - Mean Squared Error: 0.006266396908099242\n",
      "Training on chunk 6/22...\n",
      "Chunk 6 - Mean Squared Error: 0.006211923130502211\n",
      "Training on chunk 7/22...\n",
      "Chunk 7 - Mean Squared Error: 0.006263228255113702\n",
      "Training on chunk 8/22...\n",
      "Chunk 8 - Mean Squared Error: 0.006263741279470197\n",
      "Training on chunk 9/22...\n",
      "Chunk 9 - Mean Squared Error: 0.006245383318982498\n",
      "Training on chunk 10/22...\n",
      "Chunk 10 - Mean Squared Error: 0.0063311014224954635\n",
      "Training on chunk 11/22...\n",
      "Chunk 11 - Mean Squared Error: 0.0062543392603962395\n",
      "Training on chunk 12/22...\n",
      "Chunk 12 - Mean Squared Error: 0.006225541454597793\n",
      "Training on chunk 13/22...\n",
      "Chunk 13 - Mean Squared Error: 0.00631688186352749\n",
      "Training on chunk 14/22...\n",
      "Chunk 14 - Mean Squared Error: 0.006283676708144804\n",
      "Training on chunk 15/22...\n",
      "Chunk 15 - Mean Squared Error: 0.006257088849313047\n",
      "Training on chunk 16/22...\n",
      "Chunk 16 - Mean Squared Error: 0.0062668081342098685\n",
      "Training on chunk 17/22...\n",
      "Chunk 17 - Mean Squared Error: 0.006255334392154607\n",
      "Training on chunk 18/22...\n",
      "Chunk 18 - Mean Squared Error: 0.006234589505673109\n",
      "Training on chunk 19/22...\n",
      "Chunk 19 - Mean Squared Error: 0.006200574430542767\n",
      "Training on chunk 20/22...\n",
      "Chunk 20 - Mean Squared Error: 0.0062453812180898035\n",
      "Training on chunk 21/22...\n",
      "Chunk 21 - Mean Squared Error: 0.006267926816305219\n",
      "Training on chunk 22/22...\n",
      "Chunk 22 - Mean Squared Error: 0.006257930880003248\n",
      "Model training complete and saved.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Initialize the SGDRegressor model\n",
    "model = SGDRegressor()\n",
    "\n",
    "# Train separate models on each chunk\n",
    "for i in range(num_chunks):\n",
    "    print(f\"Training on chunk {i+1}/{num_chunks}...\")\n",
    "    X_chunk = joblib.load(f'ml_models/models/cities_model/encoded_features_chunk_{i+1}.pkl')\n",
    "    y_chunk = joblib.load(f'ml_models/models/cities_model/target_chunk_{i+1}.pkl')\n",
    "    \n",
    "    model.partial_fit(X_chunk, y_chunk)\n",
    "    \n",
    "    # Evaluate the model on the same chunk\n",
    "    y_pred = model.predict(X_chunk)\n",
    "    mse = mean_squared_error(y_chunk, y_pred)\n",
    "    print(f'Chunk {i+1} - Mean Squared Error: {mse}')\n",
    "\n",
    "# Save the final model\n",
    "joblib.dump(model, 'ml_models/models/cities_model/sgd_view_percentage_model.pkl')\n",
    "\n",
    "print(\"Model training complete and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        City  View Percentage\n",
      "0     Mumbai         0.182020\n",
      "1      Delhi         0.184602\n",
      "2  Bangalore         0.175156\n",
      "3  Hyderabad         0.182778\n",
      "4  Ahmedabad         0.175156\n",
      "5    Chennai         0.181023\n",
      "6    Kolkata         0.182982\n",
      "7      Surat         0.182214\n",
      "8       Pune         0.180695\n",
      "9     Jaipur         0.184509\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the trained model and encoder\n",
    "model = joblib.load('ml_models/models/cities_model/sgd_view_percentage_model.pkl')\n",
    "encoder = joblib.load('ml_models/models/cities_model/encoder.pkl')\n",
    "\n",
    "# List of major cities in India\n",
    "cities = [\n",
    "    'Mumbai', 'Delhi', 'Bangalore', 'Hyderabad', 'Ahmedabad', 'Chennai', 'Kolkata', 'Surat', 'Pune', 'Jaipur',\n",
    "    # Add more cities as needed\n",
    "]\n",
    "\n",
    "# Function to predict view percentages for a new brand\n",
    "def predict_view_percentages(brand_name, brand_category, cities, model, encoder):\n",
    "    try:\n",
    "        # Create a DataFrame with the new brand data\n",
    "        new_data = pd.DataFrame({\n",
    "            'Brand': [brand_name] * len(cities),\n",
    "            'Brand Category': [brand_category] * len(cities),\n",
    "            'City': cities\n",
    "        })\n",
    "\n",
    "        # Encode the categorical features\n",
    "        encoded_features = encoder.transform(new_data)\n",
    "        \n",
    "        # Predict view percentages\n",
    "        predictions = model.predict(encoded_features)\n",
    "        \n",
    "        # Create a DataFrame with the results\n",
    "        result_df = pd.DataFrame({\n",
    "            'City': cities,\n",
    "            'View Percentage': predictions\n",
    "        })\n",
    "        \n",
    "        return result_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "brand_name = 'Puma'\n",
    "brand_category = 'Sports'\n",
    "predicted_percentages = predict_view_percentages(brand_name, brand_category, cities, model, encoder)\n",
    "print(predicted_percentages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
